# Opera√ß√µes de Redu√ß√£o em MPI

Nesta aula, exploraremos o conceito de **redu√ß√£o distribu√≠da de dados** em ambientes de computa√ß√£o paralela utilizando a biblioteca **MPI (Message Passing Interface)**. Nosso objetivo √© compreender como distribuir tarefas entre m√∫ltiplos processos, coletar os resultados parciais e, por fim, consolidar esses resultados de forma eficiente.

Reduzir √© um conceito cl√°ssico da programa√ß√£o funcional. A redu√ß√£o de dados envolve a redu√ß√£o de um conjunto de n√∫meros em um conjunto menor de n√∫meros por meio de uma fun√ß√£o. Por exemplo, digamos que temos uma lista de n√∫meros [1, 2, 3, 4, 5]. Reduzir esta lista de n√∫meros com a fun√ß√£o sum produziria sum([1, 2, 3, 4, 5]) = 15. Da mesma forma, a redu√ß√£o da multiplica√ß√£o resultaria em multiplicar([1, 2, 3, 4, 5]) = 120.

Como voc√™ deve ter imaginado, pode ser muito complicado aplicar fun√ß√µes de redu√ß√£o em um conjunto de n√∫meros distribu√≠dos. Junto com isso, √© dif√≠cil programar de forma eficiente redu√ß√µes n√£o comutativas, ou seja, redu√ß√µes que devem ocorrer em uma ordem definida. Felizmente, o MPI tem uma fun√ß√£o √∫til chamada MPI_Reduce que ir√° lidar com quase todas as redu√ß√µes comuns que um programador precisa fazer em um aplicativo paralelo.



### Aproxima√ß√£o de PI com Comunica√ß√£o Ponto-a-Ponto

Neste exemplo, cada processo MPI (exceto o coordenador) realiza uma simula√ß√£o de Monte Carlo para estimar o valor de œÄ. Cada processo gera coordenadas aleat√≥rias (x, y) dentro do quadrado unit√°rio e verifica se o ponto est√° dentro do quarto de c√≠rculo. O n√∫mero de acertos √© enviado ao processo coordenador por meio de `MPI_Send`, que os coleta com `MPI_Recv` e calcula œÄ pela f√≥rmula ùúã ‚âà 4 \* (pontos dentro do c√≠rculo / total de pontos).

```c
#include <stdio.h>
#include <stdlib.h>
#include "mpi.h"
#include <math.h>
#define SEED_MPI 35791246

int main(int argc, char* argv[]) {
    long niter = 10000000;
    int myid, nodenum, i, count=0;
    double x, y, z, pi;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myid);
    MPI_Comm_size(MPI_COMM_WORLD, &nodenum);

    int received[nodenum];
    long recvniter[nodenum];
    srand(SEED_MPI + myid);

    if (myid != 0) {
        for (i = 0; i < niter; ++i) {
            x = ((double)rand())/RAND_MAX;
            y = ((double)rand())/RAND_MAX;
            z = sqrt(x*x + y*y);
            if (z <= 1) count++;
        }
        for (i = 0; i < nodenum; ++i) {
            MPI_Send(&count, 1, MPI_INT, 0, 1, MPI_COMM_WORLD);
            MPI_Send(&niter, 1, MPI_LONG, 0, 2, MPI_COMM_WORLD);
        }
    } else {
        for (i = 0; i < nodenum; ++i) {
            MPI_Recv(&received[i], nodenum, MPI_INT, MPI_ANY_SOURCE, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            MPI_Recv(&recvniter[i], nodenum, MPI_LONG, MPI_ANY_SOURCE, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }
        int finalcount = 0;
        long finalniter = 0;
        for (i = 0; i < nodenum; ++i) {
            finalcount += received[i];
            finalniter += recvniter[i];
        }
        pi = ((double)finalcount / (double)finalniter) * 4.0;
        printf("Pi: %f
", pi);
    }

    MPI_Finalize();
    return 0;
}
```
O programa `pi_send_receive.cpp` distribui o c√°lculo da aproxima√ß√£o de œÄ entre m√∫ltiplos processos MPI. Cada processo-executor realiza um n√∫mero fixo de itera√ß√µes e envia os resultados para o processo rank 0 usando `MPI_Send`. O rank 0 coleta esses dados com `MPI_Recv` e realiza a agrega√ß√£o final.


Use `mpicc` para compilar seus programas:

```bash
mpicc pi_send_receive.c -o pi_send_receive -lm
```

Script de Submiss√£o SLURM (exemplo: `job_pi.sh`)

```bash
#!/bin/bash
#SBATCH --job-name=mpi_pi
#SBATCH --output=output_pi.txt
#SBATCH --ntasks=16
#SBATCH --time=00:05:00
#SBATCH --partition=normal

mpirun -np $SLURM_NTASKS ./pi_send_receive
```

Submeta com:

```bash
sbatch job_pi.sh
```



### Agrega√ß√£o com MPI\_Reduce

Neste segundo exemplo, cada processo gera n√∫meros aleat√≥rios e calcula a soma local. Em vez de enviar esses valores individualmente ao rank 0, todos os processos participam de uma opera√ß√£o de redu√ß√£o coletiva com `MPI_Reduce`. A opera√ß√£o soma (`MPI_SUM`) os valores locais e entrega o total apenas ao rank 0, que ent√£o calcula a m√©dia global.

```cpp
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>
#include <assert.h>
#include <time.h>

float *create_rand_nums(int num_elements) {
  float *rand_nums = (float *)malloc(sizeof(float) * num_elements);
  assert(rand_nums != NULL);
  for (int i = 0; i < num_elements; i++) {
    rand_nums[i] = (rand() / (float)RAND_MAX);
  }
  return rand_nums;
}

int main(int argc, char** argv) {
  if (argc != 2) {
    fprintf(stderr, "Usage: avg num_elements_per_proc
");
    exit(1);
  }

  int num_elements_per_proc = atoi(argv[1]);

  MPI_Init(NULL, NULL);
  int world_rank, world_size;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  srand(time(NULL) * world_rank);
  float *rand_nums = create_rand_nums(num_elements_per_proc);

  float local_sum = 0;
  for (int i = 0; i < num_elements_per_proc; i++) local_sum += rand_nums[i];

  printf("Local sum for process %d - %f, avg = %f
", world_rank, local_sum, local_sum / num_elements_per_proc);

  float global_sum;
  MPI_Reduce(&local_sum, &global_sum, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD);

  if (world_rank == 0) {
    printf("Total sum = %f, avg = %f
", global_sum, global_sum / (world_size * num_elements_per_proc));
  }

  free(rand_nums);
  MPI_Barrier(MPI_COMM_WORLD);
  MPI_Finalize();
}
```

#### O que s√£o primitivas coletivas?

Primitivas coletivas s√£o fun√ß√µes da biblioteca MPI que envolvem todos os processos de um grupo (geralmente `MPI_COMM_WORLD`) de forma coordenada. Ao contr√°rio das comunica√ß√µes ponto-a-ponto, em que processos individuais trocam mensagens diretamente entre si (`MPI_Send` e `MPI_Recv`), as primitivas coletivas realizam opera√ß√µes em grupo ‚Äî como distribui√ß√£o de dados, coleta de resultados ou sincroniza√ß√£o.

Entre as principais primitivas coletivas, temos:

* `MPI_Bcast`: envio de dados de um processo para todos os outros
* `MPI_Gather` / `MPI_Scatter`: coleta ou distribui√ß√£o de dados entre processos
* `MPI_Reduce` / `MPI_Allreduce`: agrega√ß√£o (redu√ß√£o) de valores de todos os processos
* `MPI_Barrier`: sincroniza√ß√£o de todos os processos

Essas fun√ß√µes s√£o otimizadas internamente para oferecer alto desempenho em ambientes paralelos e facilitam a implementa√ß√£o de algoritmos distribu√≠dos.

A fun√ß√£o `MPI_Reduce` permite que cada processo envie seus dados diretamente para uma opera√ß√£o de redu√ß√£o (como soma, m√≠nimo, m√°ximo) que √© executada automaticamente no processo raiz. Isso elimina a necessidade de `MPI_Send` e `MPI_Recv` manuais, tornando o c√≥digo mais limpo e eficiente.

Este proximo exemplo estende o uso das primitivas coletivas para um cen√°rio em que **todos os processos** precisam conhecer o resultado global. Utilizamos `MPI_Allreduce` para calcular a m√©dia global e, em seguida, `MPI_Reduce` para somar os desvios quadr√°ticos ao redor da m√©dia. O desvio padr√£o √© calculado no rank0 a partir desses valores.

```cpp
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>
#include <math.h>
#include <assert.h>

float *create_rand_nums(int num_elements) {
  float *rand_nums = malloc(sizeof(float) * num_elements);
  assert(rand_nums != NULL);
  for (int i = 0; i < num_elements; i++) {
    rand_nums[i] = (rand() / (float)RAND_MAX);
  }
  return rand_nums;
}

int main(int argc, char** argv) {
  if (argc != 2) {
    fprintf(stderr, "Usage: avg num_elements_per_proc
");
    exit(1);
  }

  int num_elements_per_proc = atoi(argv[1]);

  MPI_Init(NULL, NULL);
  int world_rank, world_size;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  srand(time(NULL) * world_rank);
  float *rand_nums = create_rand_nums(num_elements_per_proc);

  float local_sum = 0;
  for (int i = 0; i < num_elements_per_proc; i++)
    local_sum += rand_nums[i];

  float global_sum;
  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD);
  float mean = global_sum / (num_elements_per_proc * world_size);

  float local_sq_diff = 0;
  for (int i = 0; i < num_elements_per_proc; i++)
    local_sq_diff += (rand_nums[i] - mean) * (rand_nums[i] - mean);

  float global_sq_diff;
  MPI_Reduce(&local_sq_diff, &global_sq_diff, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD);

  if (world_rank == 0) {
    float stddev = sqrt(global_sq_diff / (num_elements_per_proc * world_size));
    printf("Mean = %f, Standard deviation = %f
", mean, stddev);
  }

  free(rand_nums);
  MPI_Barrier(MPI_COMM_WORLD);
  MPI_Finalize();
}
```

Enquanto `MPI_Reduce` entrega o resultado da opera√ß√£o ao processo rank0, `MPI_Allreduce` entrega o mesmo resultado a todos os processos. Isso √© √∫til quando todos os n√≥s precisam do valor reduzido, como ao calcular desvios padr√£o, m√©dias globais e sincroniza√ß√µes de estado.

O c√≥digo para c√°lculo do desvio padr√£o utiliza duas redu√ß√µes: uma com `MPI_Allreduce` para obter a m√©dia global e outra com `MPI_Reduce` para calcular a soma dos desvios quadr√°ticos.



### Sua vez!

1-  Refatore `pi_send_receive.c` para utilizar `MPI_Reduce` ao inv√©s de `MPI_Send` e `MPI_Recv`.

2- Produza um c√≥digo h√≠brido MPI + OpenMP:

* O processo Rank0 calcula o n√∫mero de pontos com 16 threads OpenMP.
* Os resultados dos outros processos s√£o coletados via `MPI_Reduce`.


