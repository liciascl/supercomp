# Opera√ß√µes de Redu√ß√£o em MPI

Nesta aula, exploraremos o conceito de **redu√ß√£o distribu√≠da de dados** em ambientes de computa√ß√£o paralela utilizando a biblioteca **MPI (Message Passing Interface)**. Nosso objetivo √© compreender como distribuir tarefas entre m√∫ltiplos processos, coletar os resultados parciais e, por fim, consolidar esses resultados de forma eficiente.

Reduzir √© um conceito cl√°ssico da programa√ß√£o funcional. A redu√ß√£o de dados envolve a redu√ß√£o de um conjunto de n√∫meros em um conjunto menor de n√∫meros por meio de uma fun√ß√£o. Por exemplo, digamos que temos uma lista de n√∫meros [1, 2, 3, 4, 5]. Reduzir esta lista de n√∫meros com a fun√ß√£o sum produziria sum([1, 2, 3, 4, 5]) = 15. Da mesma forma, a redu√ß√£o da multiplica√ß√£o resultaria em multiplicar([1, 2, 3, 4, 5]) = 120.

Como voc√™ deve ter imaginado, pode ser muito complicado aplicar fun√ß√µes de redu√ß√£o em um conjunto de n√∫meros distribu√≠dos. Junto com isso, √© dif√≠cil programar de forma eficiente redu√ß√µes n√£o comutativas, ou seja, redu√ß√µes que devem ocorrer em uma ordem definida. Felizmente, o MPI tem uma fun√ß√£o √∫til chamada MPI_Reduce que ir√° lidar com quase todas as redu√ß√µes comuns que um programador precisa fazer em um aplicativo paralelo.



### Aproxima√ß√£o de PI com Comunica√ß√£o Ponto-a-Ponto

Neste exemplo, cada processo MPI (exceto o coordenador) realiza uma simula√ß√£o de Monte Carlo para estimar o valor de œÄ. Cada processo gera coordenadas aleat√≥rias (x, y) dentro do quadrado unit√°rio e verifica se o ponto est√° dentro do quarto de c√≠rculo. O n√∫mero de acertos √© enviado ao processo coordenador por meio de `MPI_Send`, que os coleta com `MPI_Recv` e calcula œÄ pela f√≥rmula ùúã ‚âà 4 \* (pontos dentro do c√≠rculo / total de pontos).

```cpp
#include <iostream>
#include <vector>
#include <cstdlib>
#include <cmath>
#include <mpi.h>

#define SEED_MPI 35791246

int main(int argc, char* argv[]) {
    const long niter = 10000000;
    int myid, nodenum, count = 0;
    double x, y, z, pi;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &myid);
    MPI_Comm_size(MPI_COMM_WORLD, &nodenum);

    std::vector<int> received(nodenum, 0);
    std::vector<long> recvniter(nodenum, 0);
    std::srand(SEED_MPI + myid);

    if (myid != 0) {
        for (long i = 0; i < niter; ++i) {
            x = static_cast<double>(std::rand()) / RAND_MAX;
            y = static_cast<double>(std::rand()) / RAND_MAX;
            z = std::sqrt(x * x + y * y);
            if (z <= 1.0) count++;
        }

        for (int i = 0; i < nodenum; ++i) {
            MPI_Send(&count, 1, MPI_INT, 0, 1, MPI_COMM_WORLD);
            MPI_Send(&niter, 1, MPI_LONG, 0, 2, MPI_COMM_WORLD);
        }
    } else {
        for (int i = 1; i < nodenum; ++i) {
            MPI_Recv(&received[i], 1, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            MPI_Recv(&recvniter[i], 1, MPI_LONG, i, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }

        int finalcount = 0;
        long finalniter = 0;
        for (int i = 1; i < nodenum; ++i) {
            finalcount += received[i];
            finalniter += recvniter[i];
        }

        pi = static_cast<double>(finalcount) / finalniter * 4.0;
        std::cout << "Pi: " << pi << std::endl;
    }

    MPI_Finalize();
    return 0;
}

```
O programa `pi_send_receive.cpp` distribui o c√°lculo da aproxima√ß√£o de œÄ entre m√∫ltiplos processos MPI. Cada processo-executor realiza um n√∫mero fixo de itera√ß√µes e envia os resultados para o processo rank 0 usando `MPI_Send`. O rank 0 coleta esses dados com `MPI_Recv` e realiza a agrega√ß√£o final.


Use `mpic++` para compilar seus programas:

```bash
mpic++ pi_send_receive.cpp -o pi_send_receive
```

Script de Submiss√£o SLURM 

```bash
#!/bin/bash
#SBATCH --job-name=mpi_pi
#SBATCH --output=output_pi.txt
#SBATCH --ntasks=16
#SBATCH --time=00:05:00
#SBATCH --partition=normal

mpirun -np $SLURM_NTASKS ./pi_send_receive
```

Submeta com:

```bash
sbatch job_pi.sh
```



### Agrega√ß√£o com MPI\_Reduce

Neste segundo exemplo, cada processo gera n√∫meros aleat√≥rios e calcula a soma local. Em vez de enviar esses valores individualmente ao rank 0, todos os processos participam de uma opera√ß√£o de redu√ß√£o coletiva com `MPI_Reduce`. A opera√ß√£o soma (`MPI_SUM`) os valores locais e entrega o total apenas ao rank 0, que ent√£o calcula a m√©dia global.

```cpp
#include <iostream>
#include <vector>
#include <cstdlib>
#include <ctime>
#include <cassert>
#include <mpi.h>

std::vector<float> create_rand_nums(int num_elements) {
    std::vector<float> rand_nums(num_elements);
    for (int i = 0; i < num_elements; ++i) {
        rand_nums[i] = static_cast<float>(std::rand()) / RAND_MAX;
    }
    return rand_nums;
}

int main(int argc, char** argv) {
    if (argc != 2) {
        std::cerr << "Usage: avg num_elements_per_proc" << std::endl;
        return 1;
    }

    int num_elements_per_proc = std::atoi(argv[1]);

    MPI_Init(nullptr, nullptr);
    int world_rank, world_size;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    std::srand(static_cast<unsigned int>(std::time(nullptr)) * (world_rank + 1));
    std::vector<float> rand_nums = create_rand_nums(num_elements_per_proc);

    float local_sum = 0.0f;
    for (float val : rand_nums) {
        local_sum += val;
    }

    std::cout << "Local sum for process " << world_rank
              << " = " << local_sum
              << ", avg = " << (local_sum / num_elements_per_proc) << std::endl;

    float global_sum = 0.0f;
    MPI_Reduce(&local_sum, &global_sum, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (world_rank == 0) {
        float global_avg = global_sum / (world_size * num_elements_per_proc);
        std::cout << "Total sum = " << global_sum
                  << ", global avg = " << global_avg << std::endl;
    }

    MPI_Barrier(MPI_COMM_WORLD);
    MPI_Finalize();
    return 0;
}

```

#### O que s√£o primitivas coletivas?

Primitivas coletivas s√£o fun√ß√µes da biblioteca MPI que envolvem todos os processos de um grupo (geralmente `MPI_COMM_WORLD`) de forma coordenada. Ao contr√°rio das comunica√ß√µes ponto-a-ponto, em que processos individuais trocam mensagens diretamente entre si (`MPI_Send` e `MPI_Recv`), as primitivas coletivas realizam opera√ß√µes em grupo ‚Äî como distribui√ß√£o de dados, coleta de resultados ou sincroniza√ß√£o.

Entre as principais primitivas coletivas, temos:

* `MPI_Bcast`: envio de dados de um processo para todos os outros
* `MPI_Gather` / `MPI_Scatter`: coleta ou distribui√ß√£o de dados entre processos
* `MPI_Reduce` / `MPI_Allreduce`: agrega√ß√£o (redu√ß√£o) de valores de todos os processos
* `MPI_Barrier`: sincroniza√ß√£o de todos os processos

Essas fun√ß√µes s√£o otimizadas internamente para oferecer alto desempenho em ambientes paralelos e facilitam a implementa√ß√£o de algoritmos distribu√≠dos.

A fun√ß√£o `MPI_Reduce` permite que cada processo envie seus dados diretamente para uma opera√ß√£o de redu√ß√£o (como soma, m√≠nimo, m√°ximo) que √© executada automaticamente no processo raiz. Isso elimina a necessidade de `MPI_Send` e `MPI_Recv` manuais, tornando o c√≥digo mais limpo e eficiente.

Este proximo exemplo estende o uso das primitivas coletivas para um cen√°rio em que **todos os processos** precisam conhecer o resultado global. Utilizamos `MPI_Allreduce` para calcular a m√©dia global e, em seguida, `MPI_Reduce` para somar os desvios quadr√°ticos ao redor da m√©dia. O desvio padr√£o √© calculado no rank0 a partir desses valores.

```cpp
#include <iostream>
#include <vector>
#include <cstdlib>
#include <cmath>
#include <ctime>
#include <cassert>
#include <mpi.h>

std::vector<float> create_rand_nums(int num_elements) {
    std::vector<float> rand_nums(num_elements);
    for (int i = 0; i < num_elements; ++i) {
        rand_nums[i] = static_cast<float>(std::rand()) / RAND_MAX;
    }
    return rand_nums;
}

int main(int argc, char** argv) {
    if (argc != 2) {
        std::cerr << "Usage: avg num_elements_per_proc" << std::endl;
        return 1;
    }

    int num_elements_per_proc = std::atoi(argv[1]);

    MPI_Init(nullptr, nullptr);
    int world_rank, world_size;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    std::srand(static_cast<unsigned int>(std::time(nullptr)) * (world_rank + 1));
    std::vector<float> rand_nums = create_rand_nums(num_elements_per_proc);

    float local_sum = 0.0f;
    for (float num : rand_nums)
        local_sum += num;

    float global_sum = 0.0f;
    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD);
    float mean = global_sum / (num_elements_per_proc * world_size);

    float local_sq_diff = 0.0f;
    for (float num : rand_nums)
        local_sq_diff += (num - mean) * (num - mean);

    float global_sq_diff = 0.0f;
    MPI_Reduce(&local_sq_diff, &global_sq_diff, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (world_rank == 0) {
        float stddev = std::sqrt(global_sq_diff / (num_elements_per_proc * world_size));
        std::cout << "Mean = " << mean << ", Standard deviation = " << stddev << std::endl;
    }

    MPI_Barrier(MPI_COMM_WORLD);
    MPI_Finalize();
    return 0;
}

```

Enquanto `MPI_Reduce` entrega o resultado da opera√ß√£o ao processo rank0, `MPI_Allreduce` entrega o mesmo resultado a todos os processos. Isso √© √∫til quando todos os n√≥s precisam do valor reduzido, como ao calcular desvios padr√£o, m√©dias globais e sincroniza√ß√µes de estado.

O c√≥digo para c√°lculo do desvio padr√£o utiliza duas redu√ß√µes: uma com `MPI_Allreduce` para obter a m√©dia global e outra com `MPI_Reduce` para calcular a soma dos desvios quadr√°ticos.



### Sua vez!

1-  Refatore o c√≥digo do primeiro exemplo para utilizar `MPI_Reduce` ao inv√©s de `MPI_Send` e `MPI_Recv`.

2- Fa√ßa um c√≥digo que utilize MPI e OpenMP em que:
    * O processo Rank0 calcula o n√∫mero de pontos com 16 threads OpenMP.
    * Os resultados dos outros processos s√£o coletados via `MPI_Reduce`.



**Fa√ßa a sua subimiss√£o at√© as 23h59 de 19/05** 

